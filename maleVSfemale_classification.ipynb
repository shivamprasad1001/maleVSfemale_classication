{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import  cv2 as cv\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ImageDataGenerator(rescale= 1/255)\n",
    "test = ImageDataGenerator(rescale= 1/255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:/my progrAMS/python/MachineLearning/DataSet/manVSwoman'"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'D:/my progrAMS/python/MachineLearning/DataSet/manVSwoman'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 953 images belonging to 2 classes.\n",
      "Found 201 images belonging to 2 classes.\n",
      "Found 12 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train.flow_from_directory('D:/my progrAMS/python/MachineLearning/DataSet/manVSwoman/traing',\n",
    "                                          target_size=(224, 224),\n",
    "                                          batch_size = 3,\n",
    "                                          class_mode='binary')\n",
    "test_dataset = test.flow_from_directory('D:/my progrAMS/python/MachineLearning/DataSet/manVSwoman/testing',\n",
    "                                          target_size=(224, 224),\n",
    "                                          batch_size = 3,\n",
    "                                          class_mode='binary')\n",
    "valid_dataset = test.flow_from_directory('D:/my progrAMS/python/MachineLearning/DataSet/manVSwoman/validation',\n",
    "                                          target_size=(224, 224),\n",
    "                                          batch_size = 3,\n",
    "                                          class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'man': 0, 'woman': 1}"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shiva\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(16, (3,3), activation = 'relu', input_shape = (224,224,3)),\n",
    "    MaxPool2D(2,2),\n",
    "\n",
    "    Conv2D(32, (3,3), activation='relu'),\n",
    "    MaxPool2D(2,2),\n",
    "\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPool2D(2,2),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "optimizer = \"adam\",\n",
    "metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shiva\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 401ms/step - accuracy: 0.5398 - loss: 0.8551 - val_accuracy: 0.5224 - val_loss: 0.7217\n",
      "Epoch 2/30\n",
      "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 383ms/step - accuracy: 0.6418 - loss: 0.6297 - val_accuracy: 0.5124 - val_loss: 0.6953\n",
      "Epoch 3/30\n",
      "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 388ms/step - accuracy: 0.7425 - loss: 0.5089 - val_accuracy: 0.6368 - val_loss: 0.7401\n",
      "Epoch 4/30\n",
      "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 340ms/step - accuracy: 0.8274 - loss: 0.3538 - val_accuracy: 0.6070 - val_loss: 0.7521\n",
      "Epoch 5/30\n",
      "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 321ms/step - accuracy: 0.8850 - loss: 0.2284 - val_accuracy: 0.6716 - val_loss: 1.3025\n",
      "Epoch 6/30\n",
      "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 313ms/step - accuracy: 0.9457 - loss: 0.1281 - val_accuracy: 0.6418 - val_loss: 2.1539\n",
      "Epoch 7/30\n",
      "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 328ms/step - accuracy: 0.9424 - loss: 0.1542 - val_accuracy: 0.7214 - val_loss: 1.8102\n",
      "Epoch 8/30\n",
      "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 335ms/step - accuracy: 0.9632 - loss: 0.0680 - val_accuracy: 0.6965 - val_loss: 2.1167\n",
      "Epoch 9/30\n",
      "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 337ms/step - accuracy: 0.9763 - loss: 0.0582 - val_accuracy: 0.7065 - val_loss: 1.9968\n",
      "Epoch 10/30\n",
      "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 321ms/step - accuracy: 0.9924 - loss: 0.0186 - val_accuracy: 0.7264 - val_loss: 2.3323\n",
      "Epoch 11/30\n",
      "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 332ms/step - accuracy: 0.9879 - loss: 0.0251 - val_accuracy: 0.7015 - val_loss: 2.1932\n",
      "Epoch 12/30\n",
      "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 335ms/step - accuracy: 0.9910 - loss: 0.0258 - val_accuracy: 0.6866 - val_loss: 2.4367\n",
      "Epoch 13/30\n",
      "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 340ms/step - accuracy: 0.9911 - loss: 0.0407 - val_accuracy: 0.7164 - val_loss: 2.3316\n",
      "Epoch 14/30\n",
      "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 323ms/step - accuracy: 0.9799 - loss: 0.0797 - val_accuracy: 0.6716 - val_loss: 2.2023\n",
      "Epoch 15/30\n",
      "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 358ms/step - accuracy: 0.9857 - loss: 0.0990 - val_accuracy: 0.6716 - val_loss: 2.9475\n",
      "Epoch 16/30\n",
      "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 351ms/step - accuracy: 0.9901 - loss: 0.0177 - val_accuracy: 0.6617 - val_loss: 3.7218\n",
      "Epoch 17/30\n",
      "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 339ms/step - accuracy: 0.9810 - loss: 0.0695 - val_accuracy: 0.6816 - val_loss: 2.8214\n",
      "Epoch 18/30\n",
      "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 345ms/step - accuracy: 0.9941 - loss: 0.0121 - val_accuracy: 0.6915 - val_loss: 3.1463\n",
      "Epoch 19/30\n",
      "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 337ms/step - accuracy: 0.9977 - loss: 0.0061 - val_accuracy: 0.6667 - val_loss: 3.0609\n",
      "Epoch 20/30\n",
      "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 392ms/step - accuracy: 0.9960 - loss: 0.0107 - val_accuracy: 0.6716 - val_loss: 3.5649\n",
      "Epoch 21/30\n",
      "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 357ms/step - accuracy: 0.9995 - loss: 0.0057 - val_accuracy: 0.6816 - val_loss: 3.8794\n",
      "Epoch 22/30\n",
      "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 356ms/step - accuracy: 0.9995 - loss: 0.0026 - val_accuracy: 0.6965 - val_loss: 4.2205\n",
      "Epoch 23/30\n",
      "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 352ms/step - accuracy: 0.9990 - loss: 0.0026 - val_accuracy: 0.6866 - val_loss: 4.5389\n",
      "Epoch 24/30\n",
      "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 343ms/step - accuracy: 0.9974 - loss: 0.0047 - val_accuracy: 0.7015 - val_loss: 4.7046\n",
      "Epoch 25/30\n",
      "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 390ms/step - accuracy: 0.9975 - loss: 0.0060 - val_accuracy: 0.6766 - val_loss: 4.3595\n",
      "Epoch 26/30\n",
      "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 331ms/step - accuracy: 0.9989 - loss: 0.0027 - val_accuracy: 0.7015 - val_loss: 4.7879\n",
      "Epoch 27/30\n",
      "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 338ms/step - accuracy: 0.9941 - loss: 0.0102 - val_accuracy: 0.6915 - val_loss: 4.3847\n",
      "Epoch 28/30\n",
      "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 341ms/step - accuracy: 0.9977 - loss: 0.0039 - val_accuracy: 0.6915 - val_loss: 4.4650\n",
      "Epoch 29/30\n",
      "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 339ms/step - accuracy: 0.9974 - loss: 0.0040 - val_accuracy: 0.6915 - val_loss: 4.6070\n",
      "Epoch 30/30\n",
      "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 336ms/step - accuracy: 0.9991 - loss: 0.0024 - val_accuracy: 0.6965 - val_loss: 4.6449\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x210a92e1150>"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, \n",
    "          epochs=30,\n",
    "          validation_data=test_dataset)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - accuracy: 0.9992 - loss: 0.0015\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.003569230902940035\n",
      "test accuracy: 0.9979013800621033\n"
     ]
    }
   ],
   "source": [
    "print(f\"test loss: {test_loss}\")\n",
    "print(f\"test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved Successfullyll..\n"
     ]
    }
   ],
   "source": [
    "model.save('new_maleVSfemaleClassification.h5')\n",
    "print('Model saved Successfullyll..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path ='D:/my progrAMS/python/MachineLearning/DataSet/manVSwoman/validation'\n",
    "test_Images_paths =[]\n",
    "for i in os.listdir(dir_path):\n",
    "    img_path = f\"{dir_path}/{i}\"\n",
    "    for j in os.listdir(img_path):\n",
    "        img_path1 = f\"{img_path}/{j}\"\n",
    "        test_Images_paths.append(img_path1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "ind = random.randint(0,len(test_Images_paths))\n",
    "ran_Img_path = test_Images_paths[ind]\n",
    "imagel = cv.imread(ran_Img_path)\n",
    "img = cv.cvtColor(imagel, cv.COLOR_RGB2BGR)\n",
    "img = cv.resize(img, (224,224) )\n",
    "plt.imshow(img)\n",
    "image = load_img(f'{ran_Img_path}', target_size=(224, 224))  # Adjust the size as per your model\n",
    "image = img_to_array(image)\n",
    "image = np.expand_dims(image, axis=0)\n",
    "\n",
    "y = model.predict(image)\n",
    "\n",
    "y_pred = y>0.5\n",
    "if y_pred == 0:\n",
    "    print(\"man\")\n",
    "\n",
    "else:\n",
    "    print(\"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved Successfullyll..\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cam = cv.VideoCapture(0)\n",
    "# while True:\n",
    "#     rat, frame = cam.read()\n",
    "#     cv.imshow(\"Camra\", frame)\n",
    "    \n",
    "#     image = load_img(frame, target_size=(224, 224))  # Adjust the size as per your model\n",
    "#     image = img_to_array(image)\n",
    "#     image = image / 255.0\n",
    "#     image = np.expand_dims(image, axis=0) # Add batch dimension (1, 224, 224, 3)\n",
    "\n",
    "#     y = model.predict(image)\n",
    "#     # print(y*100)\n",
    "#     y_pred = y>0.5\n",
    "#     if y_pred==0:\n",
    "#         print(f\"Model Perdict 'man' with {y[0][0]*100}%\")\n",
    "#     else:\n",
    "#         print(f\"Model Perdict 'woman' with {y[0][0]*100} %\")\n",
    "\n",
    "#     if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "# cam.release()\n",
    "# cv.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
